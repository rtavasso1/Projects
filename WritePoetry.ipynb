{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a raw text file of my poetry, and process it minimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Every Day a Poem.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "# remove all non-ascii characters\n",
    "text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "# remove dates\n",
    "text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', ' ', text)\n",
    "text = text[26:] # remove filler at the start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we encode our data using GPT-2's tokenizer, and defining training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "data = torch.tensor(enc.encode(text))\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0,len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our homemade transformer using masked self-attention that is capable of generating text recursively. To create the model, I borrow the rough architechture seen in GPT-2 with layer norms, feed forward layers, and residual connections to create a more robust transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_embd, n_head)\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.proj_ln = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout()\n",
    "    def forward(self, x):\n",
    "        mask = (torch.triu(torch.ones(x.shape[1], x.shape[1])) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "        x = self.ln(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x, _ = self.attn(x, x, x, attn_mask=mask)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_ln(x)\n",
    "        return x\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.proj2 = nn.Linear(4*n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout()\n",
    "    def forward(self, x):\n",
    "        x = self.proj1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.proj2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(n_embd, n_head)\n",
    "        self.ff = FeedFoward(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.proj = nn.Linear(n_embd, enc.n_vocab)\n",
    "        self.tok_emb = nn.Embedding(enc.n_vocab, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "    def forward(self, x):\n",
    "        x = self.tok_emb(x)\n",
    "        x = x + self.pos_emb(torch.arange(x.shape[1]).to(device))\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "model = Transformer(128, 8, 8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfcn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 199, Train loss: 5.605, Val loss: 6.680\n",
      "Step 399, Train loss: 5.522, Val loss: 6.798\n",
      "Step 599, Train loss: 5.342, Val loss: 6.403\n",
      "Step 799, Train loss: 5.035, Val loss: 6.749\n",
      "Step 999, Train loss: 4.698, Val loss: 6.538\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    x, y = get_batch('train')\n",
    "    B, T = x.shape\n",
    "    y_hat = model(x) # (B, T, n_vocab)\n",
    "    train_loss = lossfcn(y_hat.view(B*T,-1), y.view(B*T))\n",
    "    train_loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if (step+1) % 200 == 0:\n",
    "        model.eval()\n",
    "        x, y = get_batch('val')\n",
    "        y_hat = model(x)\n",
    "        val_loss = lossfcn(y_hat.view(B*T,-1), y.view(B*T))\n",
    "        print(f\"Step {step}, Train loss: {train_loss.item():.3f}, Val loss: {val_loss.item():.3f}\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am aya talk)\n",
      "And larger to face.\n",
      "but I am a half, but sheriff \n",
      "\n",
      "the fundamental bloodyids forgotten wonder, when you in rupt\n",
      "when top awoke.\n",
      "and forever off your above, r perception.\n",
      "but the mind through on life in her needs\n",
      "When\n",
      "and questiony o in the precip water,\n",
      "with it wasn ofing\n",
      "my ego,\n",
      "to than stick.\n",
      "as a will tracks\n",
      "itirdittingown innd end\n",
      "begin on\n",
      ".\n",
      "Fore emot Aned.\n",
      "Mr\n",
      "Mom night sat left,\n",
      "(\n",
      "I am flameki in your day itself.\n",
      "I will might like.\n",
      "\n",
      "He is not of me in the forest blame\n",
      "with.\n",
      " ander next to chase forget date, in a sound?\n",
      " \n",
      " \n",
      "\n",
      " Random is the will in the world think.\n",
      "And the tree,\n",
      " 256 boyisc buildinges,\n",
      " perennial red.\n",
      " \n",
      " \n",
      "\n",
      "d made\n"
     ]
    }
   ],
   "source": [
    "seq = torch.tensor(enc.encode('I am a')).unsqueeze(0).long().to(device)\n",
    "for step in range(200):\n",
    "    context = seq[:,-block_size:]\n",
    "    y_hat = model(context)[:,-1] # (1, n_vocab)\n",
    "    p = F.softmax(y_hat, dim=-1)\n",
    "    next_word = torch.multinomial(p, 1)\n",
    "    seq = torch.cat([seq, next_word], dim=-1)\n",
    "print(enc.decode(seq[0].cpu().numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't do very great, but there wasn't a lot of training data. We could make it a little better by having it train on a bigger set of poetry, or by leveraging a pre-trained model and fine-tuning it on my poetry. Let's do the latter. Below, I load the GPT2 model, a well-respected generative text model with public weights. It's not the best anymore, but it is still very good. I use the implementation made by Andrej Karpathy, nanoGPT, found here: https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = text[:int(n*0.9)]\n",
    "val_data = text[int(n*0.9):]\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('train.bin')\n",
    "val_ids.tofile('val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py finetune_poetry.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n",
      "\n",
      "a second time that of all in the Bible.\n",
      "The body here is only a hand, being of an earlier age.\n",
      "It is not included here as so far as I suppose the angel has a hand.\n",
      "the hand, which made up part of my heart.\n",
      "Just this is the hand that made me cry and think.\n",
      "Just this is where one lost her heart and thought.\n",
      "just this is is as long as that will last, he now has no patience.\n",
      "it\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-poetry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not super coherent, but it writes with consistent and mostly correct grammer. This could be made better by using a larger model (I used the smallest) and optimizing training data with some better cleaning and larger train set.\n",
    "\n",
    "Thanks to Andrej Karpathy for the nanoGPT implementation as well as a few bits of code like the get_batch function taken from his implementation of a transformer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weiner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d598616444d5900452a95bbb1a5944206b0527a5240a5ded3184143f27b17fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
